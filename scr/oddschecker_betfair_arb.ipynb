{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import concurrent.futures\n",
    "from selenium import webdriver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "import betfairlightweight\n",
    "from betfairlightweight import filters\n",
    "from betfairlightweight import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oddschecker data\n",
    "First extract the urls containing each of the odds tables from the oddschecker politics sitemap at https://www.oddschecker.com/sport/politics/sitemap.xml. However this does appear to be missing a bunch of markets for some reason, so it's simpler to manually update the list of markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract oddschecker politics market urls from sitemap\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") \n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "service = Service(\"/opt/homebrew/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL of the sitemap\n",
    "sitemap_url = \"https://www.oddschecker.com/sport/politics/sitemap.xml\"\n",
    "\n",
    "try:\n",
    "    # Load the sitemap page\n",
    "    driver.get(sitemap_url)\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "    xml_content = driver.page_source\n",
    "    soup = BeautifulSoup(xml_content, 'xml')\n",
    "\n",
    "    url_tags = soup.find_all('loc')\n",
    "    urls = [url_tag.text for url_tag in url_tags]\n",
    "\n",
    "    print(f\"Found {len(urls)} URLs.\")\n",
    "    print(urls)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium is then used to load the pages and grab the page source. It's currently set to run five pages concurrently (using different headers) to help speed things up. The odds table is then extracted using beautifulsoup, putting the bet name and odds for each bookmaker into a dataframe. The data for the odds of each bookmaker seems to have a somewhat random class name in the html, but most contain \"bs\" or \"o\", so this is what's searched for in the table info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract odds data from a given URL\n",
    "def extract_odds(url, user_agent):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    service = Service(\"/opt/homebrew/bin/chromedriver\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specific oddschecker page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(6) \n",
    "\n",
    "        # Get the page source\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Odds table has id \"t1\"\n",
    "        odds_table = soup.find('tbody', id='t1')\n",
    "\n",
    "        if not odds_table:\n",
    "            print(f\"No odds table found for URL: {url}\")\n",
    "            return None  # Skip this URL if the table isn't found\n",
    "\n",
    "        # Extract each row and the data within\n",
    "        odds_data = []\n",
    "        bookmakers_set = set()\n",
    "\n",
    "        for row in odds_table.find_all('tr'):\n",
    "            bet_name = row.find('a', class_='popup').text.strip() \n",
    "            odds_dict = {'Bet': bet_name}\n",
    "            \n",
    "            # Find all td elements with odds information\n",
    "            for td in row.find_all('td', class_=lambda x: x and ('o' in x.split() or 'bs' in x.split())): \n",
    "                bookmaker = td.get('data-bk')  # Extract the bookmaker name\n",
    "                decimal_odds = td.get('data-odig')  # Extract the decimal odds value\n",
    "                if bookmaker and decimal_odds:  # Only add if both are present\n",
    "                    odds_dict[bookmaker] = float(decimal_odds)  # Convert odds to float\n",
    "                    bookmakers_set.add(bookmaker)\n",
    "            \n",
    "            odds_data.append(odds_dict)\n",
    "\n",
    "        # Create a DataFrame with all bookmakers as columns\n",
    "        df = pd.DataFrame(odds_data).set_index('Bet')\n",
    "\n",
    "        # Ensure all bookmakers are columns, even if some are missing in certain rows\n",
    "        df = df.reindex(columns=sorted(bookmakers_set))\n",
    "\n",
    "        # Add the URL as a column in the DataFrame\n",
    "        df['URL'] = url\n",
    "\n",
    "        return df\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The oddschecker sitemap seems to be missing some pages, so manually updated urls\n",
    "urls = [\n",
    "    \"https://www.oddschecker.com/politics/british-politics/next-labour-leader\",\n",
    "    \"https://www.oddschecker.com/politics/british-politics/next-conservative-leader\",\n",
    "    \"https://www.oddschecker.com/politics/australian-politics/state-elections/queensland-state-election\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-presidential-election/winner\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-presidential-election/winning-party\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-presidential-election/party-of-popular-vote-winner\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-presidential-election/gender-of-election-winner\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-presidential-election/election-winner-to-lose-popular-vote\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/mississippi\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/arizona\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/massachusetts\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/oklahoma\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/pennsylvania\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/oregon\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/minnesota\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/hawaii\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/alabama\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/texas\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/rhode-island\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/florida\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/delaware\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/connecticut\",\n",
    "    \"https://www.oddschecker.com/politics/us-politics/us-state-betting/colorado\"\n",
    "]\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:87.0) Gecko/20100101 Firefox/87.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes_list_oc = []\n",
    "\n",
    "# Use ThreadPoolExecutor to process URLs in parallel in batches of 5\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for i, url in enumerate(urls):\n",
    "        user_agent = user_agents[i % len(user_agents)]  # Rotate user agents\n",
    "        futures.append(executor.submit(extract_odds, url, user_agent))\n",
    "\n",
    "        # Wait for each batch of 5 to complete before starting the next batch\n",
    "        if (i + 1) % 5 == 0 or i == len(urls) - 1:\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                df = future.result()\n",
    "                if df is not None:\n",
    "                    dataframes_list_oc.append(df)\n",
    "            futures = []  # Clear futures list for the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_oc = pd.concat(dataframes_list_oc).reset_index()\n",
    "all_data_oc.rename(columns={'index': 'Bet'}, inplace=True)\n",
    "\n",
    "print(all_data_oc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betfair data\n",
    "Next to grab the data from the Betfair Exchange API for political markets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load login credentials\n",
    "load_dotenv()\n",
    "\n",
    "bf_usr = os.getenv(\"BF_LOGIN\")\n",
    "bf_pass = os.getenv(\"BF_PASS\")\n",
    "bf_api = os.getenv(\"BF_API_KEY\")\n",
    "#bf_session = os.getenv(\"BF_SESSION\")\n",
    "bf_certs_path = '../certs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to betfair client\n",
    "client = APIClient(bf_usr, bf_pass, app_key=bf_api, certs=bf_certs_path)\n",
    "client.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The market ID's are extracted given the event ID for politcal bets, followed by getting the bid/ask price and size for each market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Market Catalogues and create mappings for Selection and Market Names\n",
    "market_filter = betfairlightweight.filters.market_filter(\n",
    "    event_type_ids=['2378961'],  # Politics event type\n",
    ")\n",
    "\n",
    "# Get market catalogues, including runners\n",
    "market_catalogues = client.betting.list_market_catalogue(\n",
    "    filter=market_filter,\n",
    "    max_results=100,  # Adjust this as needed\n",
    "    market_projection=['RUNNER_DESCRIPTION']  # Include runner descriptions to get selection names\n",
    ")\n",
    "\n",
    "# Extract market IDs and create mappings\n",
    "market_ids = [market.market_id for market in market_catalogues]\n",
    "\n",
    "selection_mapping = {}\n",
    "market_name_mapping = {}\n",
    "\n",
    "for market in market_catalogues:\n",
    "    market_name_mapping[market.market_id] = market.market_name  # Map market_id to market_name\n",
    "    for runner in market.runners:\n",
    "        selection_mapping[runner.selection_id] = runner.runner_name  # Map selection_id to selection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process runner books and include selection and market names\n",
    "def process_runner_books(runner_books, selection_mapping, market_name, market_id):\n",
    "    best_back_prices = [\n",
    "        runner_book.ex.available_to_back[0]['price'] if runner_book.ex.available_to_back else 1.01\n",
    "        for runner_book in runner_books\n",
    "    ]\n",
    "    best_back_sizes = [\n",
    "        runner_book.ex.available_to_back[0]['size'] if runner_book.ex.available_to_back else 1.01\n",
    "        for runner_book in runner_books\n",
    "    ]\n",
    "\n",
    "    best_lay_prices = [\n",
    "        runner_book.ex.available_to_lay[0]['price'] if runner_book.ex.available_to_lay else 1000.0\n",
    "        for runner_book in runner_books\n",
    "    ]\n",
    "    best_lay_sizes = [\n",
    "        runner_book.ex.available_to_lay[0]['size'] if runner_book.ex.available_to_lay else 1.01\n",
    "        for runner_book in runner_books\n",
    "    ]\n",
    "\n",
    "    selection_ids = [runner_book.selection_id for runner_book in runner_books]\n",
    "    selection_names = [selection_mapping.get(runner_book.selection_id, \"Unknown\") for runner_book in runner_books]\n",
    "    last_prices_traded = [runner_book.last_price_traded for runner_book in runner_books]\n",
    "    total_matched = [runner_book.total_matched for runner_book in runner_books]\n",
    "    statuses = [runner_book.status for runner_book in runner_books]\n",
    "    scratching_datetimes = [runner_book.removal_date for runner_book in runner_books]\n",
    "    adjustment_factors = [runner_book.adjustment_factor for runner_book in runner_books]\n",
    "\n",
    "    market_id = str(market_id)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Market ID': market_id,\n",
    "        'Market Name': market_name,\n",
    "        'Selection ID': selection_ids,\n",
    "        'Selection Name': selection_names,\n",
    "        'Best Back Price': best_back_prices,\n",
    "        'Best Back Size': best_back_sizes,\n",
    "        'Best Lay Price': best_lay_prices,\n",
    "        'Best Lay Size': best_lay_sizes,\n",
    "        'Last Price Traded': last_prices_traded,\n",
    "        'Total Matched': total_matched,\n",
    "        'Status': statuses,\n",
    "        'Removal Date': scratching_datetimes,\n",
    "        'Adjustment Factor': adjustment_factors\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a price filter for market data\n",
    "price_filter = betfairlightweight.filters.price_projection(\n",
    "    price_data=['EX_BEST_OFFERS']\n",
    ")\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes_list_bf = []\n",
    "\n",
    "# Loop through each market ID and fetch market book data\n",
    "for market_id in market_ids:\n",
    "    # Request market book for each market ID\n",
    "    market_books = client.betting.list_market_book(\n",
    "        market_ids=[market_id],\n",
    "        price_projection=price_filter\n",
    "    )\n",
    "    \n",
    "    # Ensure that market books were returned\n",
    "    if market_books:\n",
    "        # Process the first market book (only one is requested)\n",
    "        market_book = market_books[0]\n",
    "        \n",
    "        # Get market name using the market_id\n",
    "        market_name = market_name_mapping[market_id]\n",
    "        \n",
    "        # Process runner books and store in DataFrame, including selection names and market names\n",
    "        runners_df = process_runner_books(market_book.runners, selection_mapping, market_name, market_id)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes_list_bf.append(runners_df)\n",
    "\n",
    "# Optionally, you can concatenate all dataframes into a single dataframe\n",
    "all_data_df = pd.concat(dataframes_list_bf, ignore_index=True)\n",
    "\n",
    "# Display or process the combined DataFrame as needed\n",
    "print(all_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet matching\n",
    "An attempt to automate matching the market names of betfair to oddschecker using cosine similarity. It has some success but as it's unlikely that markets will be added or removed frequently, it'll probably be easier to manually match the markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract market names from Betfair data\n",
    "betfair_market_names = list(set(all_data_df['Market Name'].tolist()))\n",
    "\n",
    "# Combine all market names for vectorization\n",
    "all_market_names = betfair_market_names + urls\n",
    "\n",
    "# Vectorize the market names using TF-IDF\n",
    "vectorizer = TfidfVectorizer().fit_transform(all_market_names)\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "# Calculate cosine similarity between Betfair and Oddschecker markets\n",
    "cosine_sim_matrix = cosine_similarity(vectors[:len(betfair_market_names)], vectors[len(betfair_market_names):])\n",
    "\n",
    "# Find the best matches for each Betfair market\n",
    "matches = []\n",
    "for i, betfair_name in enumerate(betfair_market_names):\n",
    "    similarity_scores = cosine_sim_matrix[i]\n",
    "    best_match_idx = np.argmax(similarity_scores)\n",
    "    best_match_score = similarity_scores[best_match_idx]\n",
    "    best_match_name = urls[best_match_idx]\n",
    "    matches.append({\n",
    "        'Betfair Market Name': betfair_name,\n",
    "        'Oddschecker Market Name': best_match_name,\n",
    "        'Similarity Score': best_match_score\n",
    "    })\n",
    "\n",
    "# Convert matches to DataFrame for easier review\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Filter out rows with a similarity score of 0\n",
    "matches_df_filtered = matches_df[matches_df['Similarity Score'] > 0]\n",
    "\n",
    "# Sort the DataFrame by similarity score in descending order\n",
    "matches_df_filtered = matches_df_filtered.sort_values(by='Similarity Score', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(matches_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mappings are loaded from data/markets.csv as opposed to the cosine similarity matching above. A fuzzy match is then carried out between the 'Selection Name' from the betfair data and 'Bet' from the oddschecker data as the name of the Bets may not be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz import process\n",
    "\n",
    "# Load the mapping CSV file and ensure Market ID is a string for matching consistency\n",
    "mapping_df = pd.read_csv(\"../data/markets.csv\")\n",
    "mapping_df['Market ID'] = mapping_df['Market ID'].astype(str)\n",
    "\n",
    "# Load the first DataFrame (Betfair data) and ensure Market ID is a string\n",
    "betfair_df = all_data_df\n",
    "betfair_df['Market ID'] = betfair_df['Market ID'].astype(str)\n",
    "\n",
    "# Load the second DataFrame (Oddschecker data)\n",
    "oddschecker_df = all_data_oc\n",
    "\n",
    "# Normalize the 'Selection Name' and 'Bet' columns for better matching\n",
    "betfair_df['Selection Name Normalized'] = betfair_df['Selection Name'].str.lower().str.strip()\n",
    "oddschecker_df['Bet Normalized'] = oddschecker_df['Bet'].str.lower().str.strip()\n",
    "\n",
    "# Initialize a list to collect all results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over each market in the mapping DataFrame\n",
    "for index, row in mapping_df.iterrows():\n",
    "    market_id = row['Market ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Skip markets without a URL\n",
    "    if pd.isna(url) or url.strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Filter Betfair DataFrame to only include rows with the current Market ID\n",
    "    betfair_filtered_df = betfair_df[betfair_df['Market ID'] == market_id]\n",
    "    \n",
    "    # Filter Oddschecker DataFrame to only include rows with the current URL\n",
    "    oddschecker_filtered_df = oddschecker_df[oddschecker_df['URL'] == url]\n",
    "    \n",
    "    # Skip if there are no corresponding entries in either filtered DataFrame\n",
    "    if betfair_filtered_df.empty or oddschecker_filtered_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Create a function to find the best match for each 'Selection Name'\n",
    "    def match_bets(selection_name, bet_choices):\n",
    "        match, score, _ = process.extractOne(selection_name, bet_choices, scorer=fuzz.token_sort_ratio)\n",
    "        return match, score\n",
    "\n",
    "    # Apply the matching function to each 'Selection Name' in the filtered Betfair DataFrame\n",
    "    bet_choices = oddschecker_filtered_df['Bet Normalized'].tolist()\n",
    "    matches = betfair_filtered_df['Selection Name Normalized'].apply(lambda x: match_bets(x, bet_choices))\n",
    "\n",
    "    # Add match results to betfair_filtered_df\n",
    "    betfair_filtered_df['Best Match Bet'] = matches.apply(lambda x: x[0])\n",
    "    betfair_filtered_df['Similarity Score'] = matches.apply(lambda x: x[1])\n",
    "\n",
    "    # Filter matches based on a similarity threshold (e.g., 80)\n",
    "    threshold = 80\n",
    "    filtered_matches_df = betfair_filtered_df[betfair_filtered_df['Similarity Score'] >= threshold]\n",
    "    \n",
    "    # Merge with the filtered Oddschecker DataFrame on 'URL' and 'Best Match Bet'\n",
    "    final_filtered_df = pd.merge(filtered_matches_df, oddschecker_filtered_df, left_on=['Best Match Bet'], right_on=['Bet'], how='left')\n",
    "    \n",
    "    # Drop temporary columns and finalize DataFrame\n",
    "    final_filtered_df = final_filtered_df.drop(columns=['Selection Name Normalized', 'Bet Normalized', 'Best Match Bet', 'Similarity Score'])\n",
    "    \n",
    "    # Append the results to the all_results list\n",
    "    all_results.append(final_filtered_df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list to create a single DataFrame\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Display or save the final DataFrame as needed\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping CSV file and ensure Market ID is a string for matching consistency\n",
    "mapping_df = pd.read_csv(\"../data/markets.csv\")\n",
    "mapping_df['Market ID'] = mapping_df['Betfair Market ID'].astype(str)\n",
    "\n",
    "# Load the first DataFrame (Betfair data) and ensure Market ID is a string\n",
    "betfair_df = all_data_df\n",
    "betfair_df['Market ID'] = betfair_df['Market ID'].astype(str)\n",
    "\n",
    "# Load the second DataFrame (Oddschecker data)\n",
    "oddschecker_df = all_data_oc\n",
    "\n",
    "# Normalize the 'Selection Name' and 'Bet' columns for better matching\n",
    "betfair_df['Selection Name Normalized'] = betfair_df['Selection Name'].str.lower().str.strip()\n",
    "oddschecker_df['Bet Normalized'] = oddschecker_df['Bet'].str.lower().str.strip()\n",
    "\n",
    "# Initialize a list to collect all results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over each market in the mapping DataFrame\n",
    "for index, row in mapping_df.iterrows():\n",
    "    market_id = row['Market ID']\n",
    "    url = row['Oddschecker URL']\n",
    "    \n",
    "    # Skip markets without a URL\n",
    "    if pd.isna(url) or url.strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Filter Betfair DataFrame to only include rows with the current Market ID\n",
    "    betfair_filtered_df = betfair_df[betfair_df['Market ID'] == market_id]\n",
    "    \n",
    "    # Filter Oddschecker DataFrame to only include rows with the current URL\n",
    "    oddschecker_filtered_df = oddschecker_df[oddschecker_df['URL'] == url]\n",
    "    \n",
    "    # Skip if there are no corresponding entries in either filtered DataFrame\n",
    "    if betfair_filtered_df.empty or oddschecker_filtered_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Perform direct matching of 'Selection Name Normalized' with 'Bet Normalized'\n",
    "    merged_df = pd.merge(\n",
    "        betfair_filtered_df,\n",
    "        oddschecker_filtered_df,\n",
    "        left_on='Selection Name Normalized',\n",
    "        right_on='Bet Normalized',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Drop unnecessary columns from the merged DataFrame\n",
    "    merged_df = merged_df.drop(columns=['Selection Name Normalized', 'Bet Normalized'])\n",
    "    \n",
    "    # Append the results to the all_results list\n",
    "    all_results.append(merged_df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list to create a single DataFrame\n",
    "odds_df = pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_arbitrage(df):\n",
    "    odds_columns = ['AKB', 'B3', 'BF', 'BY', 'CE', 'DP', 'EE', 'FB', 'FR', 'G5', 'KN', 'LD', 'LS', 'MA', 'N4', 'OE', 'PP', 'QN', 'S6', 'SI', 'SK', 'SX', 'UN', 'VC', 'VT', 'WA', 'WH']\n",
    "    #select_columns = ['Market Name', 'Selection Name', 'Odds to Lay Ratio', 'Best Back Price', 'Best Lay Price', 'Best Lay Size', 'Best Odds', 'Best Bookmaker', 'URL']\n",
    "\n",
    "    # Find the best odds and corresponding bookmaker\n",
    "    df['Best Odds'] = df[odds_columns].max(axis=1)\n",
    "    df['Best Bookmaker'] = df.apply(lambda row: ', '.join([col for col in odds_columns if row[col] == row['Best Odds']]), axis=1)\n",
    "\n",
    "    # Calculate the ratio of Best Odds to Best Lay Price\n",
    "    df['Odds to Lay Ratio'] = df.apply(lambda row: row['Best Odds'] / row['Best Lay Price'] if row['Best Odds'] <= 500 else 0, axis=1)\n",
    "\n",
    "    # Sort the DataFrame by the 'Odds to Lay Ratio' in descending order\n",
    "    df = df.sort_values(by='Odds to Lay Ratio', ascending=False)\n",
    "\n",
    "    # Convert necessary columns to float for calculations\n",
    "    df['Best Odds'] = df['Best Odds'].astype(float)\n",
    "    df['Best Lay Price'] = df['Best Lay Price'].astype(float)\n",
    "    df['Best Lay Size'] = df['Best Lay Size'].astype(float)\n",
    "\n",
    "    # Calculate Lay Liability\n",
    "    df['Lay Liability'] = (df['Best Lay Size'] * (df['Best Lay Price'] - 1)).round(2)\n",
    "\n",
    "    # Calculate Back Amount (B) for hedging\n",
    "    df['Back Amount'] = (df['Best Lay Size'] * df['Best Lay Price'] / (df['Best Odds'])).round(2)\n",
    "\n",
    "    # Calculate Profit\n",
    "    df['Profit If Outcome Happens'] = (df['Back Amount'] * (df['Best Odds'] - 1)) - df['Lay Liability']\n",
    "    df['Profit If Outcome Does Not Happen'] = df['Best Lay Size'] - df['Back Amount']\n",
    "\n",
    "    # Set Profit to the minimum of the two scenarios, rounded to 2 decimal places\n",
    "    df['Profit'] = df[['Profit If Outcome Happens', 'Profit If Outcome Does Not Happen']].min(axis=1).round(2)\n",
    "\n",
    "    # Set Profit to 0 if Best Odds > 500\n",
    "    df.loc[df['Best Odds'] > 500, 'Profit'] = 0\n",
    "\n",
    "    # Filter for potential arbitrage opportunities\n",
    "    arb_opportunities = df[df['Odds to Lay Ratio'] > 1]\n",
    "\n",
    "    # Select and order the columns for the final DataFrame\n",
    "    cols = ['Market Name', 'Selection Name', 'Odds to Lay Ratio', 'Best Odds', 'Best Lay Price', 'Best Lay Size', 'Best Bookmaker',\n",
    "            'Lay Liability', 'Back Amount', 'Profit', 'URL']\n",
    "\n",
    "    return arb_opportunities[cols]\n",
    "\n",
    "arbitrage_opportunities = identify_arbitrage(odds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(arbitrage_opportunities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
